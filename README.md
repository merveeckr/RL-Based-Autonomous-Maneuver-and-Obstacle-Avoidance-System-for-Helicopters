# ğŸš 3D Helicopter Flight Control with Reinforcement Learning

Bu proje, Reinforcement Learning (RL) kullanarak 3D ortamda helikopter kontrolÃ¼ Ã¶ÄŸrenen bir sistemdir. Helikopter, engellerden kaÃ§Ä±narak hedefe ulaÅŸmayÄ± Ã¶ÄŸrenir. Sistem, gerÃ§ek FlightGear simÃ¼lasyon verilerini kullanarak gerÃ§ekÃ§i helikopter davranÄ±ÅŸÄ± saÄŸlar.

---

## ğŸ› ï¸ Kurulum

### Gereksinimler

- Python 3.8+
- CUDA (GPU iÃ§in, opsiyonel)

### AdÄ±m 1: BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle

pip install -r requirements.txt
### AdÄ±m 2: Veri DosyasÄ±nÄ± Kontrol Et

`fg_log2.csv` dosyasÄ±nÄ±n proje dizininde olduÄŸundan emin olun. Bu dosya FlightGear telemetry verilerini iÃ§erir ve gerÃ§ekÃ§i helikopter davranÄ±ÅŸÄ± iÃ§in kullanÄ±lÄ±r.

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### 1. Environment'i Test Et

python flight_env_3d.pyBu komut environment'i test eder ve 50 rastgele adÄ±m Ã§alÄ±ÅŸtÄ±rÄ±r. 3D gÃ¶rselleÅŸtirme aÃ§Ä±lÄ±r.

### 2. Model EÄŸit

python train_3d_ppo.py --total_timesteps 200000 --model_name my_helicopter_model### 3. Modeli Test Et

python test_3d_environment.py \
    --model_path ./models_3d/my_helicopter_model_best/best_model.zip \
    --num_episodes 5---

## ğŸ® Environment NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r

### Manuel Test (Kod ile)

from flight_env_3d import FlightControlEnv3D
import numpy as np

# Environment oluÅŸtur
env = FlightControlEnv3D(
    world_size=(500.0, 500.0, 200.0),  # DÃ¼nya boyutlarÄ± (x, y, z) metre
    num_obstacles=1,                     # Engel sayÄ±sÄ±
    max_episode_steps=2000,              # Maksimum adÄ±m sayÄ±sÄ±
    render_mode='human',                 # GÃ¶rselleÅŸtirme: 'human' veya None
    use_log_data=True,                   # FlightGear log verilerini kullan
    log_data_path='fg_log2.csv',         # Log dosyasÄ± yolu
    moving_obstacles=False,              # Hareketli engeller (False = sabit)
    target_behind_obstacle=True         # Target engelin arkasÄ±nda
)

# Environment'i sÄ±fÄ±rla
obs, info = env.reset(seed=42)

print(f"Initial position: {info['position']}")
print(f"Target position: {info['target']}")
print(f"Distance to target: {info['distance_to_target']:.2f}m")

# Rastgele aksiyonlar ile test et
for step in range(100):
    action = env.action_space.sample()  # Rastgele aksiyon
    obs, reward, terminated, truncated, info = env.step(action)
    
    print(f"Step {step}: Reward={reward:.2f}, Distance={info['distance_to_target']:.2f}m")
    
    if terminated:
        print("Collision detected!")
        break
    if truncated:
        print("Timeout!")
        break
    
    # GÃ¶rselleÅŸtir
    env.render()

env.close()### Environment Parametreleri

| Parametre | Tip | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|-----|------------|----------|
| `world_size` | Tuple | (500, 500, 200) | 3D dÃ¼nya boyutlarÄ± (x, y, z) metre |
| `num_obstacles` | int | 1 | Engel sayÄ±sÄ± (ÅŸu an sadece 1 sabit engel) |
| `max_episode_steps` | int | 2000 | Maksimum adÄ±m sayÄ±sÄ± |
| `render_mode` | str | None | 'human' = gÃ¶rselleÅŸtirme, None = gÃ¶rselleÅŸtirme yok |
| `use_log_data` | bool | False | FlightGear log verilerini kullan |
| `log_data_path` | str | 'fg_log2.csv' | Log dosyasÄ± yolu |
| `moving_obstacles` | bool | False | Hareketli engeller (ÅŸu an False) |
| `target_behind_obstacle` | bool | True | Target engelin arkasÄ±nda |

### State Space (GÃ¶zlem)

Environment 10 boyutlu state vektÃ¶rÃ¼ dÃ¶ndÃ¼rÃ¼r:
on
state = [x, y, z, vx, vy, vz, roll, pitch, yaw, altitude_rate]
#        |  position  |  velocity  |    attitude    |  rate- **Position**: `[x, y, z]` - Helikopter pozisyonu (metre)
- **Velocity**: `[vx, vy, vz]` - HÄ±z vektÃ¶rÃ¼ (m/s)
- **Attitude**: `[roll, pitch, yaw]` - AÃ§Ä±lar (derece)
- **Altitude Rate**: `altitude_rate` - Ä°rtifa deÄŸiÅŸim hÄ±zÄ± (m/s)

### Action Space (Aksiyon)

Environment 4 boyutlu aksiyon vektÃ¶rÃ¼ kabul eder:

action = [roll_command, pitch_command, yaw_command, throttle_command]
#         |              attitude commands              |  throttle- **Roll Command**: YatÄ±ÅŸ komutu (-1.0 ile 1.0 arasÄ±)
- **Pitch Command**: YÃ¼kseliÅŸ/dalÄ±ÅŸ komutu (-1.0 ile 1.0 arasÄ±)
- **Yaw Command**: DÃ¶nÃ¼ÅŸ komutu (-1.0 ile 1.0 arasÄ±)
- **Throttle Command**: Gaz komutu (-1.0 ile 1.0 arasÄ±)

---

## ğŸ“ Model EÄŸitimi ve Kaydetme

### Temel EÄŸitim

python train_3d_ppo.py --total_timesteps 200000 --model_name my_model### GeliÅŸmiÅŸ EÄŸitim (Daha Fazla Parametre)

python train_3d_ppo.py \
    --total_timesteps 500000 \
    --learning_rate 3e-4 \
    --num_obstacles 1 \
    --model_name advanced_model \
    --log_dir ./logs_3d/ \
    --save_dir ./models_3d/### EÄŸitim Parametreleri

| Parametre | VarsayÄ±lan | AÃ§Ä±klama |
|-----------|------------|----------|
| `--total_timesteps` | 200000 | Toplam eÄŸitim adÄ±mÄ± |
| `--learning_rate` | 3e-4 | Ã–ÄŸrenme oranÄ± |
| `--num_obstacles` | 1 | Engel sayÄ±sÄ± |
| `--model_name` | Otomatik | Model adÄ± (tarih-saat ile) |
| `--log_dir` | ./logs_3d/ | TensorBoard log dizini |
| `--save_dir` | ./models_3d/ | Model kayÄ±t dizini |

### Model Kaydetme

EÄŸitim sÄ±rasÄ±nda otomatik olarak 3 tip model kaydedilir:

1. **Best Model** (En Ä°yi Model)
   - Yolu: `./models_3d/{model_name}_best/best_model.zip`
   - AÃ§Ä±klama: Evaluation sÄ±rasÄ±nda en yÃ¼ksek reward'a sahip model
   - **KullanÄ±m**: Test iÃ§in en iyi seÃ§enek

2. **Checkpoints** (Ara KayÄ±tlar)
   - Yolu: `./models_3d/{model_name}_checkpoints/{model_name}_50000_steps.zip`
   - AÃ§Ä±klama: Belirli adÄ±m sayÄ±larÄ±nda kaydedilen modeller
   - KullanÄ±m: EÄŸitimi yarÄ±da kesip devam ettirmek iÃ§in

3. **Final Model** (Final Model)
   - Yolu: `./models_3d/{model_name}_final.zip`
   - AÃ§Ä±klama: EÄŸitimin sonunda kaydedilen model
   - KullanÄ±m: Son durumu saklamak iÃ§in


python test_3d_environment.py \
    --model_path ./models_3d/my_helicopter_model_best/best_model.zip \
    --num_episodes 5z (â˜…)**: Target
- **KÄ±rmÄ±zÄ± Silindir**: Engel
- **Mavi Ã‡izgi**: Helikopter trajectory'si

---

## ğŸ“ Kod DosyalarÄ± ve AÃ§Ä±klamalarÄ±

### 1. `flight_env_3d.py` - 3D Environment

**Ne Ä°ÅŸe Yarar:**
- 3D helikopter uÃ§uÅŸ ortamÄ±nÄ± tanÄ±mlar
- Gymnasium-compatible environment saÄŸlar
- Engel kaÃ§Ä±nma, collision detection, reward hesaplama yapar

**Ana SÄ±nÄ±flar:**
- `Obstacle3D`: 3D engel tanÄ±mÄ± (silindir/kÃ¼re)
- `FlightControlEnv3D`: Ana environment sÄ±nÄ±fÄ±

**Ã–nemli Metodlar:**
- `reset()`: Environment'i sÄ±fÄ±rla, yeni episode baÅŸlat
- `step(action)`: Bir adÄ±m ilerle, reward dÃ¶ndÃ¼r
- `render()`: 3D gÃ¶rselleÅŸtirme
- `_check_collisions()`: Ã‡arpÄ±ÅŸma kontrolÃ¼
- `_generate_obstacles()`: Engel oluÅŸturma

**KullanÄ±m:**
python
from flight_env_3d import FlightControlEnv3D
env = FlightControlEnv3D(render_mode='human')---

### 2. `train_3d_ppo.py` - Model EÄŸitimi

**Ne Ä°ÅŸe Yarar:**
- PPO (Proximal Policy Optimization) algoritmasÄ± ile model eÄŸitir
- Stable-Baselines3 kullanÄ±r
- Otomatik model kaydetme ve evaluation yapar

**Ana Fonksiyonlar:**
- `train_ppo_3d()`: EÄŸitim fonksiyonu
- `make_env_3d()`: Environment factory

**Ã–zellikler:**
- Otomatik best model kaydetme
- Checkpoint kaydetme (belirli adÄ±mlarda)
- TensorBoard logging
- Evaluation callback

**KullanÄ±m:**
python train_3d_ppo.py --total_timesteps 200000 --model_name my_model---

### 3. `test_3d_environment.py` - Model Testi

**Ne Ä°ÅŸe Yarar:**
- EÄŸitilmiÅŸ modeli test eder
- SenaryolarÄ± Ã§alÄ±ÅŸtÄ±rÄ±r
- Performans metrikleri toplar

**Ana Fonksiyon:**
- `test_model_3d()`: Test fonksiyonu

**Toplanan Metrikler:**
- Episode rewards
- Episode lengths
- Success rate
- Goal reached count
- Collision count

**KullanÄ±m:**
python test_3d_environment.py --model_path ./models_3d/model_best/best_model.zip---

### 4. `state_extractor.py` - State Extraction

**Ne Ä°ÅŸe Yarar:**
- FlightGear log verilerinden state vektÃ¶rÃ¼ Ã§Ä±karÄ±r
- Collision detection yapar
- State bounds hesaplar

**Ana SÄ±nÄ±f:**
- `StateExtractor`: State extraction sÄ±nÄ±fÄ±

**Ã‡Ä±kardÄ±ÄŸÄ± State:**hon
state = [altitude_agl, roll, pitch, heading, altitude_rate]**Metodlar:**
- `load_data()`: CSV dosyasÄ±nÄ± yÃ¼kle
- `extract_states()`: State vektÃ¶rlerini Ã§Ä±kar
- `detect_collisions()`: Ã‡arpÄ±ÅŸma tespiti
- `get_state_bounds()`: State sÄ±nÄ±rlarÄ±

**KullanÄ±m:**n
from state_extractor import StateExtractor
extractor = StateExtractor()
df = extractor.load_data('fg_log2.csv')
states = extractor.extract_states(df)---

### 5. `reward_design.py` - Reward Function

**Ne Ä°ÅŸe Yarar:**
- Reward function tasarÄ±mÄ± ve hesaplama
- Reward bileÅŸenlerini analiz eder
- GÃ¶rselleÅŸtirme saÄŸlar

**Ana SÄ±nÄ±f:**
- `RewardDesigner`: Reward tasarÄ±m sÄ±nÄ±fÄ±

**Reward BileÅŸenleri:**
reward = R_altitude + R_stability + R_collision + R_goal + R_progress- **R_altitude**: GÃ¼venli irtifa iÃ§in Ã¶dÃ¼l
- **R_stability**: Roll/pitch stabilitesi iÃ§in ceza
- **R_collision**: Ã‡arpÄ±ÅŸma iÃ§in bÃ¼yÃ¼k ceza
- **R_goal**: Hedefe ulaÅŸma iÃ§in Ã¶dÃ¼l
- **R_progress**: Hedefe yaklaÅŸma iÃ§in Ã¶dÃ¼l

**Metodlar:**
- `compute_reward()`: Reward hesapla
- `compute_batch_rewards()`: Toplu reward hesaplama
- `visualize_rewards()`: Reward gÃ¶rselleÅŸtirme

**KullanÄ±m:**
from reward_design import RewardDesigner
designer = RewardDesigner()
reward = designer.compute_reward(state, collision=False)---

### 6. `analyze_helicopter_behavior.py` - Behavior Analysis

**Ne Ä°ÅŸe Yarar:**
- EÄŸitilmiÅŸ modelin davranÄ±ÅŸÄ±nÄ± analiz eder
- GerÃ§ek helikopter log verileriyle karÅŸÄ±laÅŸtÄ±rÄ±r
- Similarity score hesaplar

**Ana SÄ±nÄ±f:**
- `HelicopterBehaviorAnalyzer`: Analiz sÄ±nÄ±fÄ±

**Analiz Metrikleri:**
- Roll/Pitch angle similarity
- Altitude rate similarity
- Speed similarity
- Angular rates similarity
- Overall similarity score

**Ã‡Ä±ktÄ±lar:**
- `behavior_comparison.png`: GÃ¶rselleÅŸtirme
- `behavior_report.txt`: DetaylÄ± rapor

**KullanÄ±m:**
python analyze_helicopter_behavior.py \
    --model_path ./models_3d/model_best/best_model.zip \
    --n_episodes 20---

### 7. `improve_with_log_data.py` - Log Data Analysis

**Ne Ä°ÅŸe Yarar:**
- Log verisi kalitesini analiz eder
- Ä°yileÅŸtirme Ã¶nerileri sunar
- Enhanced environment config Ã¶nerir

**Ana SÄ±nÄ±f:**
- `LogDataEnhancer`: Log verisi analiz sÄ±nÄ±fÄ±

**Analiz Edilenler:**
- Veri hacmi ve Ã§eÅŸitlilik
- UÃ§uÅŸ fazlarÄ± (climbing, level, descending)
- Maneuver Ã§eÅŸitliliÄŸi
- Kalite Ã¶nerileri

**KullanÄ±m:**
python improve_with_log_data.py---

## ğŸ“Š Analiz AraÃ§larÄ±

### 1. Behavior Analysis

Modelin gerÃ§ek helikopter gibi davranÄ±p davranmadÄ±ÄŸÄ±nÄ± kontrol edin:

python analyze_helicopter_behavior.py \
    --model_path ./models_3d/my_model_best/best_model.zip \
    --n_episodes 20 \
    --output_dir ./behavior_analysis/**Ã‡Ä±ktÄ±lar:**
- Similarity scores (0-100%)
- GÃ¶rselleÅŸtirmeler
- DetaylÄ± rapor

### 2. Log Data Quality

Log verisi kalitesini analiz edin:

python improve_with_log_data.py**Ã‡Ä±ktÄ±lar:**
- Veri kalitesi analizi
- Ä°yileÅŸtirme Ã¶nerileri
- Enhanced config Ã¶nerileri

---

## â“ SÄ±k Sorulan Sorular

### Q: Model nerede kaydediliyor?

A: Modeller `./models_3d/` dizininde kaydedilir:
- **Best model**: `./models_3d/{model_name}_best/best_model.zip`
- **Checkpoints**: `./models_3d/{model_name}_checkpoints/`
- **Final**: `./models_3d/{model_name}_final.zip`

### Q: EÄŸitimi yarÄ±da kesip devam ettirebilir miyim?

A: Evet! Checkpoint'lerden devam edebilirsiniz:

from stable_baselines3 import PPO

# Checkpoint'ten yÃ¼kle
model = PPO.load('./models_3d/my_model_checkpoints/my_model_200000_steps.zip')

# EÄŸitime devam et
model.learn(total_timesteps=300000)  # Toplam 500k olacak### Q: GÃ¶rselleÅŸtirme nasÄ±l kapatÄ±lÄ±r?

A: `render_mode=None` kullanÄ±n veya test scriptinde `--render` parametresini kullanmayÄ±n.

### Q: FarklÄ± engel sayÄ±larÄ± ile eÄŸitebilir miyim?

A: Åu an sadece 1 sabit engel destekleniyor. Kodda `num_obstacles=1` olarak ayarlanmÄ±ÅŸ.

### Q: Log verileri nasÄ±l kullanÄ±lÄ±yor?

A: Log verileri baÅŸlangÄ±Ã§ durumlarÄ± iÃ§in kullanÄ±lÄ±yor:
- Initial attitude (roll, pitch, heading)
- Initial velocity
- GerÃ§ekÃ§i helikopter davranÄ±ÅŸÄ± saÄŸlar

### Q: Similarity score ne anlama geliyor?

A: Similarity score (0-100%), modelin gerÃ§ek helikopter davranÄ±ÅŸÄ±na ne kadar benzediÄŸini gÃ¶sterir:
- **>70%**: Ä°yi benzerlik âœ…
- **50-70%**: Orta benzerlik âš ï¸
- **<50%**: DÃ¼ÅŸÃ¼k benzerlik âŒ

### Q: TensorBoard'da ne gÃ¶rÃ¼rÃ¼m?

A: TensorBoard'da ÅŸunlarÄ± gÃ¶rÃ¼rsÃ¼nÃ¼z:
- Episode reward
- Episode length
- Learning rate
- Policy loss
- Value loss
- Evaluation metrics

---

## ğŸ“š Ek Kaynaklar

- **3D Environment Guide**: `3D_ENVIRONMENT_GUIDE.md`
- **Training Guide**: `TRAINING_GUIDE.md`
- **Behavior Analysis Guide**: `BEHAVIOR_ANALYSIS_GUIDE.md`

---

## ğŸ› Sorun Giderme

### Problem: "Model not found"

**Ã‡Ã¶zÃ¼m:** Model yolunu kontrol edin:
ls ./models_3d/my_model_best/### Problem: "Import error"

**Ã‡Ã¶zÃ¼m:** BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:
pip install -r requirements.txt### Problem: GÃ¶rselleÅŸtirme aÃ§Ä±lmÄ±yor

**Ã‡Ã¶zÃ¼m:** `render_mode='human'` kullandÄ±ÄŸÄ±nÄ±zdan emin olun ve matplotlib backend'inin doÄŸru olduÄŸunu kontrol edin.

### Problem: EÄŸitim Ã§ok yavaÅŸ

**Ã‡Ã¶zÃ¼m:** 
- GPU kullanÄ±n (CUDA)
- `total_timesteps` deÄŸerini azaltÄ±n
- `n_steps` parametresini ayarlayÄ±n

---

